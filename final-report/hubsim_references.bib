
@misc{cheng_compost_2023,
	title = {{CoMPosT}: {Characterizing} and {Evaluating} {Caricature} in {LLM} {Simulations}},
	shorttitle = {{CoMPosT}},
	url = {http://arxiv.org/abs/2310.11501},
	abstract = {Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.},
	urldate = {2023-10-26},
	publisher = {arXiv},
	author = {Cheng, Myra and Piccardi, Tiziano and Yang, Diyi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11501 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
	annote = {Comment: To appear at EMNLP 2023 (Main)},
	file = {arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\TKPLP3MY\\2310.html:text/html;Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\9FHTKEHC\\Cheng et al. - 2023 - CoMPosT Characterizing and Evaluating Caricature .pdf:application/pdf},
}

@misc{huang_benchmarking_2023,
	title = {Benchmarking {Large} {Language} {Models} {As} {AI} {Research} {Agents}},
	url = {http://arxiv.org/abs/2310.03302},
	doi = {10.48550/arXiv.2310.03302},
	abstract = {Scientific experimentation involves an iterative process of creating hypotheses, designing experiments, running experiments, and analyzing the results. Can we build AI research agents to perform these long-horizon tasks? To take a step towards building and evaluating research agents on such open-ended decision-making tasks, we focus on the problem of machine learning engineering: given a task description and a dataset, build a high-performing model. In this paper, we propose MLAgentBench, a suite of ML tasks for benchmarking AI research agents. Agents can perform actions like reading/writing files, executing code, and inspecting outputs. With these actions, agents could run experiments, analyze the results, and modify the code of entire machine learning pipelines, such as data processing, architecture, training processes, etc. The benchmark then automatically evaluates the agent's performance objectively over various metrics related to performance and efficiency. We also design an LLM-based research agent to automatically perform experimentation loops in such an environment. Empirically, we find that a GPT-4-based research agent can feasibly build compelling ML models over many tasks in MLAgentBench, displaying highly interpretable plans and actions. However, the success rates vary considerably; they span from almost 90{\textbackslash}\% on well-established older datasets to as low as 10{\textbackslash}\% on recent Kaggle Challenges -- unavailable during the LLM model's pretraining -- and even 0{\textbackslash}\% on newer research challenges like BabyLM. Finally, we identify several key challenges for LLM-based research agents such as long-term planning and hallucination. Our code is released at https://github.com/snap-stanford/MLAgentBench.},
	urldate = {2023-10-26},
	publisher = {arXiv},
	author = {Huang, Qian and Vora, Jian and Liang, Percy and Leskovec, Jure},
	month = oct,
	year = {2023},
	note = {arXiv:2310.03302 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\DR7NTXCT\\Huang et al. - 2023 - Benchmarking Large Language Models As AI Research .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\BNSMAJE9\\2310.html:text/html},
}

@misc{park_generative_2023,
	title = {Generative {Agents}: {Interactive} {Simulacra} of {Human} {Behavior}},
	shorttitle = {Generative {Agents}},
	url = {http://arxiv.org/abs/2304.03442},
	doi = {10.48550/arXiv.2304.03442},
	abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = aug,
	year = {2023},
	note = {arXiv:2304.03442 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	annote = {

Does the memory stream store one stream per Agent separately?


},
	file = {arXiv Fulltext PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\I7RR25F6\\Park et al. - 2023 - Generative Agents Interactive Simulacra of Human .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\F4XLAGCA\\2304.html:text/html},
}

@misc{tornberg_simulating_2023,
	title = {Simulating {Social} {Media} {Using} {Large} {Language} {Models} to {Evaluate} {Alternative} {News} {Feed} {Algorithms}},
	url = {http://arxiv.org/abs/2310.05984},
	abstract = {Social media is often criticized for amplifying toxic discourse and discouraging constructive conversations. But designing social media platforms to promote better conversations is inherently challenging. This paper asks whether simulating social media through a combination of Large Language Models (LLM) and Agent-Based Modeling can help researchers study how different news feed algorithms shape the quality of online conversations. We create realistic personas using data from the American National Election Study to populate simulated social media platforms. Next, we prompt the agents to read and share news articles - and like or comment upon each other's messages - within three platforms that use different news feed algorithms. In the first platform, users see the most liked and commented posts from users whom they follow. In the second, they see posts from all users - even those outside their own network. The third platform employs a novel "bridging" algorithm that highlights posts that are liked by people with opposing political views. We find this bridging algorithm promotes more constructive, non-toxic, conversation across political divides than the other two models. Though further research is needed to evaluate these findings, we argue that LLMs hold considerable potential to improve simulation research on social media and many other complex social settings.},
	urldate = {2023-10-12},
	publisher = {arXiv},
	author = {T{\"o}rnberg, Petter and Valeeva, Diliara and Uitermark, Justus and Bail, Christopher},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05984 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Computer Science - Social and Information Networks},
	file = {arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\EV2R2B4K\\2310.html:text/html;Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\DMQHR5TD\\T{\"o}rnberg et al. - 2023 - Simulating Social Media Using Large Language Model.pdf:application/pdf},
}

@misc{qian_communicative_2023,
	title = {Communicative {Agents} for {Software} {Development}},
	url = {http://arxiv.org/abs/2307.07924},
	abstract = {Software engineering is a domain characterized by intricate decision-making processes, often relying on nuanced intuition and consultation. Recent advancements in deep learning have started to revolutionize software engineering practices through elaborate designs implemented at various stages of software development. In this paper, we present an innovative paradigm that leverages large language models (LLMs) throughout the entire software development process, streamlining and unifying key processes through natural language communication, thereby eliminating the need for specialized models at each phase. At the core of this paradigm lies ChatDev, a virtual chat-powered software development company that mirrors the established waterfall model, meticulously dividing the development process into four distinct chronological stages: designing, coding, testing, and documenting. Each stage engages a team of agents, such as programmers, code reviewers, and test engineers, fostering collaborative dialogue and facilitating a seamless workflow. The chat chain acts as a facilitator, breaking down each stage into atomic subtasks. This enables dual roles, allowing for proposing and validating solutions through context-aware communication, leading to efficient resolution of specific subtasks. The instrumental analysis of ChatDev highlights its remarkable efficacy in software generation, enabling the completion of the entire software development process in under seven minutes at a cost of less than one dollar. It not only identifies and alleviates potential vulnerabilities but also rectifies potential hallucinations while maintaining commendable efficiency and cost-effectiveness. The potential of ChatDev unveils fresh possibilities for integrating LLMs into the realm of software development.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Qian, Chen and Cong, Xin and Liu, Wei and Yang, Cheng and Chen, Weize and Su, Yusheng and Dang, Yufan and Li, Jiahao and Xu, Juyuan and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
	month = aug,
	year = {2023},
	note = {arXiv:2307.07924 [cs]
version: 3},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Multiagent Systems},
	annote = {Comment: https://github.com/OpenBMB/ChatDev},
	file = {arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\MXZP7FKR\\2307.html:text/html;Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\UNB2UZHW\\Qian et al. - 2023 - Communicative Agents for Software Development.pdf:application/pdf},
}

@misc{wu_autogen_2023,
	title = {{AutoGen}: {Enabling} {Next}-{Gen} {LLM} {Applications} via {Multi}-{Agent} {Conversation}},
	shorttitle = {{AutoGen}},
	url = {https://arxiv.org/abs/2308.08155v2},
	abstract = {AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.},
	language = {en},
	urldate = {2023-10-10},
	journal = {arXiv.org},
	author = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and Awadallah, Ahmed Hassan and White, Ryen W. and Burger, Doug and Wang, Chi},
	month = aug,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\5GZ4PHI4\\Wu et al. - 2023 - AutoGen Enabling Next-Gen LLM Applications via Mu.pdf:application/pdf},
}

@misc{wang_rolellm_2023,
	title = {{RoleLLM}: {Benchmarking}, {Eliciting}, and {Enhancing} {Role}-{Playing} {Abilities} of {Large} {Language} {Models}},
	shorttitle = {{RoleLLM}},
	url = {http://arxiv.org/abs/2310.00746},
	abstract = {The advent of Large Language Models (LLMs) has paved the way for complex tasks such as role-playing, which enhances user interactions by enabling models to imitate various characters. However, the closed-source nature of state-of-the-art LLMs and their general-purpose training limit role-playing optimization. In this paper, we introduce RoleLLM, a framework to benchmark, elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four stages: (1) Role Profile Construction for 100 roles; (2) Context-Based Instruction Generation (Context-Instruct) for role-specific knowledge extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning open-source models along with role customization. By Context-Instruct and RoleGPT, we create RoleBench, the first systematic and fine-grained character-level benchmark dataset for role-playing with 168,093 samples. Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese), significantly enhancing role-playing abilities and even achieving comparable results with RoleGPT (using GPT-4).},
	urldate = {2023-11-17},
	publisher = {arXiv},
	author = {Wang, Zekun Moore and Peng, Zhongyuan and Que, Haoran and Liu, Jiaheng and Zhou, Wangchunshu and Wu, Yuhan and Guo, Hongcheng and Gan, Ruitong and Ni, Zehao and Zhang, Man and Zhang, Zhaoxiang and Ouyang, Wanli and Xu, Ke and Chen, Wenhu and Fu, Jie and Peng, Junran},
	month = oct,
	year = {2023},
	note = {arXiv:2310.00746 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 30 pages, repo at https://github.com/InteractiveNLP-Team/RoleLLM-public},
	file = {arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\CB68RPBV\\2310.html:text/html;Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\Q37ZTVJS\\Wang et al. - 2023 - RoleLLM Benchmarking, Eliciting, and Enhancing Ro.pdf:application/pdf},
}

@misc{wang_does_2023,
	title = {Does {Role}-{Playing} {Chatbots} {Capture} the {Character} {Personalities}? {Assessing} {Personality} {Traits} for {Role}-{Playing} {Chatbots}},
	shorttitle = {Does {Role}-{Playing} {Chatbots} {Capture} the {Character} {Personalities}?},
	url = {http://arxiv.org/abs/2310.17976},
	abstract = {The emergence of large-scale pretrained language models has revolutionized the capabilities of new AI application, especially in the realm of crafting chatbots with distinct personas. Given the "stimulus-response" nature of chatbots, this paper unveils an innovative open-ended interview-style approach for personality assessment on role-playing chatbots, which offers a richer comprehension of their intrinsic personalities. We conduct personality assessments on 32 role-playing chatbots created by the ChatHaruhi library, across both the Big Five and MBTI dimensions, and measure their alignment with human perception. Evaluation results underscore that modern role-playing chatbots based on LLMs can effectively portray personality traits of corresponding characters, with an alignment rate of 82.8\% compared with human-perceived personalities. Besides, we also suggest potential strategies for shaping chatbots' personalities. Hence, this paper serves as a cornerstone study for role-playing chatbots that intersects computational linguistics and psychology. Our resources are available at https://github.com/LC1332/Chat-Haruhi-Suzumiya},
	urldate = {2023-11-17},
	publisher = {arXiv},
	author = {Wang, Xintao and Tu, Quan and Fei, Yaying and Leng, Ziang and Li, Cheng},
	month = oct,
	year = {2023},
	note = {arXiv:2310.17976 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: A Personality Traits Test Over ChatHaruhi},
	file = {arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\Z2QXMVHX\\2310.html:text/html;Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\43BFMKDF\\Wang et al. - 2023 - Does Role-Playing Chatbots Capture the Character P.pdf:application/pdf},
}

@misc{davidson_evaluating_2024,
	title = {Evaluating {Language} {Model} {Agency} through {Negotiations}},
	url = {http://arxiv.org/abs/2401.04536},
	abstract = {Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source models are currently unable to complete these tasks; (ii) cooperative bargaining games prove challenging; and (iii) the most powerful models do not always "win".},
	urldate = {2024-01-11},
	publisher = {arXiv},
	author = {Davidson, Tim R. and Veselovsky, Veniamin and Josifoski, Martin and Peyrard, Maxime and Bosselut, Antoine and Kosinski, Michal and West, Robert},
	month = jan,
	year = {2024},
	note = {arXiv:2401.04536 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Code and link to project data are made available at https://github.com/epfl-dlab/LAMEN},
	file = {arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\PRSJ9DG6\\2401.html:text/html;Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\UXKWY6ZM\\Davidson et al. - 2024 - Evaluating Language Model Agency through Negotiati.pdf:application/pdf},
}

@article{shanahan_role_2023,
	title = {Role play with large language models},
	volume = {623},
	copyright = {2023 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06647-8},
	doi = {10.1038/s41586-023-06647-8},
	abstract = {As dialogue agents become increasingly human-like in their performance, we must develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. Here we foreground the concept of role play. Casting dialogue-agent behaviour in terms of role play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models that they in fact lack. Two important cases of dialogue-agent behaviour are addressed this way, namely, (apparent) deception and (apparent) self-awareness.},
	language = {en},
	number = {7987},
	urldate = {2024-01-30},
	journal = {Nature},
	author = {Shanahan, Murray and McDonell, Kyle and Reynolds, Laria},
	month = nov,
	year = {2023},
	note = {Number: 7987
Publisher: Nature Publishing Group},
	keywords = {Computer science, Philosophy},
	pages = {493--498},
	file = {Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\E4EAA8WC\\Shanahan et al. - 2023 - Role play with large language models.pdf:application/pdf},
}

@misc{liu_agentbench_2023,
	title = {{AgentBench}: {Evaluating} {LLMs} as {Agents}},
	shorttitle = {{AgentBench}},
	url = {http://arxiv.org/abs/2308.03688},
	doi = {10.48550/arXiv.2308.03688},
	abstract = {Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Training on code and high quality multi-turn alignment data could improve agent performance. Datasets, environments, and an integrated evaluation package for AgentBench are released at {\textbackslash}url\{https://github.com/THUDM/AgentBench\}.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and Zhang, Shudan and Deng, Xiang and Zeng, Aohan and Du, Zhengxiao and Zhang, Chenhui and Shen, Sheng and Zhang, Tianjun and Su, Yu and Sun, Huan and Huang, Minlie and Dong, Yuxiao and Tang, Jie},
	month = oct,
	year = {2023},
	note = {arXiv:2308.03688 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 55 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\7HCQP8LB\\Liu et al. - 2023 - AgentBench Evaluating LLMs as Agents.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\KGE6BU6I\\2308.html:text/html},
}

@inproceedings{zhang_trojaning_2021,
	title = {Trojaning {Language} {Models} for {Fun} and {Profit}},
	url = {https://ieeexplore.ieee.org/abstract/document/9581257},
	doi = {10.1109/EuroSP51992.2021.00022},
	abstract = {Recent years have witnessed the emergence of a new paradigm of building natural language processing (NLP) systems: general-purpose, pre-trained language models (LMs) are composed with simple downstream models and fine-tuned for a variety of NLP tasks. This paradigm shift significantly simplifies the system development cycles. However, as many LMs are provided by untrusted third parties, their lack of standardization or regulation entails profound security implications, which are largely unexplored. To bridge this gap, this work studies the security threats posed by malicious LMs to NLP systems. Specifically, we present TrojanLM, a new class of trojaning attacks in which maliciously crafted LMs trigger host NLP systems to malfunction in a highly predictable manner. By empirically studying three state-of-the-art LMs (BERT, GPT-2, XLNet) in a range of security-critical NLP tasks (toxic comment detection, question answering, text completion) as well as user studies on crowdsourcing platforms, we demonstrate that TrojanLM possesses the following properties: (i) flexibility - the adversary is able to flexibly define logical combinations (e.g., {\textquoteleft}and{\textquoteright}, {\textquoteleft}or{\textquoteright}, {\textquoteleft}xor{\textquoteright}) of arbitrary words as triggers, (ii) efficacy - the host systems misbehave as desired by the adversary with high probability when {\textquotedblleft}trigger{\textquotedblright} -embedded inputs are present, (iii) specificity - the trojan LMs function indistinguishably from their benign counterparts on clean inputs, and (iv) fluency - the trigger-embedded inputs appear as fluent natural language and highly relevant to their surrounding contexts. We provide analytical justification for the practicality of TrojanLM, and further discuss potential countermeasures and their challenges, which lead to several promising research directions.},
	urldate = {2024-02-05},
	booktitle = {2021 {IEEE} {European} {Symposium} on {Security} and {Privacy} ({EuroS}\&{P})},
	author = {Zhang, Xinyang and Zhang, Zheng and Ji, Shouling and Wang, Ting},
	month = sep,
	year = {2021},
	keywords = {Natural language processing, Bridges, Buildings, Crowdsourcing, Knowledge discovery, Regulation, Standardization},
	pages = {179--197},
}

@misc{guo_gpt_2023,
	title = {{GPT} in {Game} {Theory} {Experiments}},
	url = {http://arxiv.org/abs/2305.05516},
	doi = {10.48550/arXiv.2305.05516},
	abstract = {This paper explores the use of Generative Pre-trained Transformers (GPT) in strategic game experiments, specifically the ultimatum game and the prisoner's dilemma. I designed prompts and architectures to enable GPT to understand the game rules and to generate both its choices and the reasoning behind decisions. The key findings show that GPT exhibits behaviours similar to human responses, such as making positive offers and rejecting unfair ones in the ultimatum game, along with conditional cooperation in the prisoner's dilemma. The study explores how prompting GPT with traits of fairness concern or selfishness influences its decisions. Notably, the "fair" GPT in the ultimatum game tends to make higher offers and reject offers more frequently compared to the "selfish" GPT. In the prisoner's dilemma, high cooperation rates are maintained only when both GPT players are "fair". The reasoning statements GPT produces during gameplay reveal the underlying logic of certain intriguing patterns observed in the games. Overall, this research shows the potential of GPT as a valuable tool in social science research, especially in experimental studies and social simulations.},
	urldate = {2024-03-06},
	publisher = {arXiv},
	author = {Guo, Fulin},
	month = dec,
	year = {2023},
	note = {arXiv:2305.05516 [econ, q-fin]},
	keywords = {Economics - General Economics},
	annote = {Comment: updated to use GPT-4 instead of GPT-3.5 and added reasoning analysis},
	file = {arXiv Fulltext PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\7VQQLJKW\\Guo - 2023 - GPT in Game Theory Experiments.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\WRG74ES7\\2305.html:text/html},
}

@misc{fu_improving_2023,
	title = {Improving {Language} {Model} {Negotiation} with {Self}-{Play} and {In}-{Context} {Learning} from {AI} {Feedback}},
	url = {http://arxiv.org/abs/2305.10142},
	doi = {10.48550/arXiv.2305.10142},
	abstract = {We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing, reflecting, and criticizing. We are interested in this question because if LLMs were able to improve each other, it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other, playing the roles of a buyer and a seller, respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model, playing the critic, provides feedback to a player to improve the player's negotiation strategies. We let the two agents play multiple rounds, using previous negotiation history and AI feedback as in-context demonstrations to improve the model's negotiation strategy iteratively. We use different LLMs (GPT and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing findings: (1) Only a subset of the language models we consider can self-play and improve the deal price from AI feedback, weaker models either do not understand the game's rules or cannot incorporate AI feedback for further improvement. (2) Models' abilities to learn from the feedback differ when playing different roles. For example, it is harder for Claude-instant to improve as the buyer than as the seller. (3) When unrolling the game to multiple rounds, stronger agents can consistently improve their performance by meaningfully using previous experiences and iterative AI feedback, yet have a higher risk of breaking the deal. We hope our work provides insightful initial explorations of having models autonomously improve each other with game playing and AI feedback.},
	urldate = {2024-03-06},
	publisher = {arXiv},
	author = {Fu, Yao and Peng, Hao and Khot, Tushar and Lapata, Mirella},
	month = may,
	year = {2023},
	note = {arXiv:2305.10142 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Preprint. Code at https://github.com/FranxYao/GPT-Bargaining},
	file = {arXiv Fulltext PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\ZXU6C9A7\\Fu et al. - 2023 - Improving Language Model Negotiation with Self-Pla.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\DD3QKDZK\\2305.html:text/html},
}

@misc{brookins_playing_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Playing {Games} {With} {GPT}: {What} {Can} {We} {Learn} {About} a {Large} {Language} {Model} {From} {Canonical} {Strategic} {Games}?},
	shorttitle = {Playing {Games} {With} {GPT}},
	url = {https://papers.ssrn.com/abstract=4493398},
	doi = {10.2139/ssrn.4493398},
	abstract = {We aim to understand fundamental preferences over fairness and cooperation embedded in artificial intelligence (AI). We do this by having a large language model (LLM), GPT-3.5, play two classic games: the dictator game and the prisoner's dilemma. We compare the decisions of the LLM to those of humans in laboratory experiments. We find that the LLM replicates human tendencies towards fairness and cooperation. It does not choose the optimal strategy in most cases. Rather, it shows a tendency towards fairness in the dictator game, even more so than human participants. In the prisoner's dilemma, the LLM displays rates of cooperation much higher than human participants (about 65\% versus 37\% for humans). These findings aid our understanding of the ethics and rationality embedded in AI.},
	language = {en},
	urldate = {2024-03-09},
	author = {Brookins, Philip and DeBacker, Jason Matthew},
	month = jun,
	year = {2023},
	keywords = {AI, Experimental Economics, Game Theory, Generative Pre-trained Transformer (GPT), Large language models (LLMs)},
}

@misc{liu_agentlite_2024,
	title = {{AgentLite}: {A} {Lightweight} {Library} for {Building} and {Advancing} {Task}-{Oriented} {LLM} {Agent} {System}},
	shorttitle = {{AgentLite}},
	url = {http://arxiv.org/abs/2402.15538},
	abstract = {The booming success of LLMs initiates rapid development in LLM agents. Though the foundation of an LLM agent is the generative model, it is critical to devise the optimal reasoning strategies and agent architectures. Accordingly, LLM agent research advances from the simple chain-of-thought prompting to more complex ReAct and Reflection reasoning strategy; agent architecture also evolves from single agent generation to multi-agent conversation, as well as multi-LLM multi-agent group chat. However, with the existing intricate frameworks and libraries, creating and evaluating new reasoning strategies and agent architectures has become a complex challenge, which hinders research investigation into LLM agents. Thus, we open-source a new AI agent library, AgentLite, which simplifies this process by offering a lightweight, user-friendly platform for innovating LLM agent reasoning, architectures, and applications with ease. AgentLite is a task-oriented framework designed to enhance the ability of agents to break down tasks and facilitate the development of multi-agent systems. Furthermore, we introduce multiple practical applications developed with AgentLite to demonstrate its convenience and flexibility. Get started now at: {\textbackslash}url\{https://github.com/SalesforceAIResearch/AgentLite\}.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Liu, Zhiwei and Yao, Weiran and Zhang, Jianguo and Yang, Liangwei and Liu, Zuxin and Tan, Juntao and Choubey, Prafulla K. and Lan, Tian and Wu, Jason and Wang, Huan and Heinecke, Shelby and Xiong, Caiming and Savarese, Silvio},
	month = feb,
	year = {2024},
	note = {arXiv:2402.15538 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	annote = {Comment: preprint. Library is available at https://github.com/SalesforceAIResearch/AgentLite},
	file = {arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\4BLQFEX7\\2402.html:text/html;Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\KL3RYESH\\Liu et al. - 2024 - AgentLite A Lightweight Library for Building and .pdf:application/pdf},
}

@misc{moura_joaomdmouracrewai_2024,
	title = {joaomdmoura/{crewAI}},
	copyright = {MIT},
	url = {https://github.com/joaomdmoura/crewAI},
	abstract = {Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.},
	urldate = {2024-03-09},
	author = {Moura, Jo{\~a}o},
	month = mar,
	year = {2024},
	note = {original-date: 2023-10-27T03:26:59Z},
	keywords = {ai, agents, ai-agents, llms},
}

@misc{chase_langchain_2022,
	title = {{LangChain}},
	copyright = {MIT},
	url = {https://github.com/langchain-ai/langchain},
	abstract = {???? Build context-aware reasoning applications},
	urldate = {2024-03-09},
	author = {Chase, Harrison},
	month = oct,
	year = {2022},
	note = {original-date: 2022-10-17T02:58:36Z},
}

@misc{johnschulman_introducing_2022,
	title = {Introducing {ChatGPT}},
	url = {https://openai.com/blog/chatgpt},
	abstract = {We{\textquoteright}ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
	language = {en-US},
	urldate = {2023-05-30},
	author = {{John,Schulman} and {Barret,Zoph} and {Christina,Kim} and {Jacob,Hilton} and {Jacob,Menick} and {Jiayi,Weng} and {Juan Felipe Ceron,Uribe} and {Liam,Fedus} and {Luke,Metz} and {Michael,Pokorny} and {Rapha Gontijo,Lopes} and {Shengjia,Zhao} and {Arun,Vijayvergiya} and {Eric,Sigler} and {Adam,Perelman} and {Chelsea,Voss} and {Mike,Heaton} and {Joel,Parish} and {Dave,Cummings} and {Rajeev,Nayak} and {Valerie,Balcom} and {David,Schnurr} and {Tomer,Kaftan} and {Chris,Hallacy} and {Nicholas,Turley} and {Noah,Deutsch} and {Vik,Goel} and {Jonathan,Ward} and {Aris,Konstantinidis} and {Wojciech,Zaremba} and {Long,Ouyang} and {Leonard,Bogdonoff} and {Joshua,Gross} and {David,Medina} and {Sarah,Yoo} and {Teddy,Lee} and {Ryan,Lowe} and {Dan,Mossing} and {Joost,Huizinga} and {Roger,Jiang} and {Carroll,Wainwright} and {Diogo,Almeida} and {Steph,Lin} and {Marvin,Zhang} and {Kai,Xiao} and {Katarina,Slama} and {Steven,Bills} and {Alex,Gray} and {Jan,Leike} and {Jakub,Pachocki} and {Phil,Tillet} and {Shantanu,Jain} and {Greg,Brockman} and {Nick,Ryder} and {Alex,Paino} and {Qiming,Yuan} and {Clemens,Winter} and {Ben,Wang} and {Mo,Bavarian} and {Igor,Babuschkin} and {Szymon,Sidor} and {Ingmar,Kanitscheider} and {Mikhail,Pavlov} and {Matthias,Plappert} and {Nik,Tezak} and {Heewoo,Jun} and {William,Zhuk} and {Vitchyr,Pong} and {Lukasz,Kaiser} and {Jerry,Tworek} and {Andrew,Carr} and {Lilian,Weng} and {Sandhini,Agarwal} and {Karl,Cobbe} and {Vineet,Kosaraju} and {Alethea,Power} and {Stanislas,Polu} and {Jesse,Han} and {Raul,Puri} and {Shawn,Jain} and {Benjamin,Chess} and {Christian,Gibson} and {Oleg,Boiko} and {Emy,Parparita} and {Amin,Tootoonchian} and {Kyle,Kosic} and {Christopher,Hesse}},
	month = nov,
	year = {2022},
	file = {Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\RXVN9YP3\\chatgpt.html:text/html;Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\T8QK7RJ2\\chatgpt.html:text/html},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-05-30},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 100 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\XX6YTHXM\\OpenAI - 2023 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\V59XLL4Q\\2303.html:text/html},
}

@misc{anthropic_introducing_2023,
	title = {Introducing {Claude}},
	url = {https://www.anthropic.com/index/introducing-claude},
	abstract = {After working with key partners for the past few months, we{\textquoteright}re opening up access to Claude, our AI assistant.},
	language = {en},
	urldate = {2023-05-30},
	journal = {Anthropic},
	author = {{Anthropic}},
	month = mar,
	year = {2023},
}

@misc{noauthor_models_nodate,
	title = {Models},
	url = {https://docs.cohere.com/docs/models},
	abstract = {Cohere has a variety of models that cover many different use cases. If you need more customization, you can train a model to tune it to your specific use case.},
	language = {en},
	urldate = {2024-03-09},
	journal = {Cohere AI},
	file = {Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\QPSSZ7S3\\models.html:text/html},
}

@misc{touvron_llama_2023,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\VJ5Y8JBX\\Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\9MBBA8WM\\2307.html:text/html},
}

@misc{ai_yi_2024,
	title = {Yi: {Open} {Foundation} {Models} by 01.{AI}},
	shorttitle = {Yi},
	url = {http://arxiv.org/abs/2403.04652},
	doi = {10.48550/arXiv.2403.04652},
	abstract = {We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model. We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance. We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance. We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {AI, 01 and Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and Yu, Kaidong and Liu, Peng and Liu, Qiang and Yue, Shawn and Yang, Senbin and Yang, Shiming and Yu, Tao and Xie, Wen and Huang, Wenhao and Hu, Xiaohui and Ren, Xiaoyi and Niu, Xinyao and Nie, Pengcheng and Xu, Yuchi and Liu, Yudong and Wang, Yue and Cai, Yuxuan and Gu, Zhenyu and Liu, Zhiyuan and Dai, Zonghong},
	month = mar,
	year = {2024},
	note = {arXiv:2403.04652 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\WX2LFHGX\\AI et al. - 2024 - Yi Open Foundation Models by 01.AI.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\TNVE6IV3\\2403.html:text/html},
}

@misc{jiang_mixtral_2024,
	title = {Mixtral of {Experts}},
	url = {http://arxiv.org/abs/2401.04088},
	abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, L{\'e}lio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Th{\'e}ophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
	month = jan,
	year = {2024},
	note = {arXiv:2401.04088 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: See more details at https://mistral.ai/news/mixtral-of-experts/},
	file = {arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\JYFNR2RS\\2401.html:text/html;Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\RVFM84VN\\Jiang et al. - 2024 - Mixtral of Experts.pdf:application/pdf},
}

@misc{noauthor_unalignmenttoxic-dpo-v02_2024,
	title = {unalignment/toxic-dpo-v0.2 {\textperiodcentered} {Datasets} at {Hugging} {Face}},
	url = {https://huggingface.co/datasets/unalignment/toxic-dpo-v0.2},
	abstract = {We{\textquoteright}re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-03-09},
	month = feb,
	year = {2024},
}

@misc{noauthor_jondurbinbagel-dpo-34b-v02_2024,
	title = {jondurbin/bagel-dpo-34b-v0.2 {\textperiodcentered} {Hugging} {Face}},
	url = {https://huggingface.co/jondurbin/bagel-dpo-34b-v0.2},
	abstract = {We{\textquoteright}re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-03-09},
	month = mar,
	year = {2024},
}

@misc{noauthor_bwunicluster20_nodate,
	title = {{BwUniCluster2}.0 - {bwHPC} {Wiki}},
	url = {https://wiki.bwhpc.de/e/BwUniCluster2.0},
	urldate = {2024-03-09},
}

@misc{noauthor_open_nodate,
	title = {Open {LLM} {Leaderboard} - a {Hugging} {Face} {Space} by {HuggingFaceH4}},
	url = {https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard},
	abstract = {Discover amazing ML apps made by the community},
	urldate = {2024-03-10},
	file = {Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\Z4IPK4IR\\open_llm_leaderboard.html:text/html},
}

@misc{noauthor_lm-sysfastchat_2024,
	title = {lm-sys/{FastChat}},
	copyright = {Apache-2.0},
	url = {https://github.com/lm-sys/FastChat},
	abstract = {An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena.},
	urldate = {2024-03-10},
	publisher = {LMSYS},
	month = mar,
	year = {2024},
	note = {original-date: 2023-03-19T00:18:02Z},
}

@misc{bianchi_how_2024,
	title = {How {Well} {Can} {LLMs} {Negotiate}? {NegotiationArena} {Platform} and {Analysis}},
	shorttitle = {How {Well} {Can} {LLMs} {Negotiate}?},
	url = {http://arxiv.org/abs/2402.05863},
	doi = {10.48550/arXiv.2402.05863},
	abstract = {Negotiation is the basis of social interactions; humans negotiate everything from the price of cars to how to share common resources. With rapidly growing interest in using large language models (LLMs) to act as agents on behalf of human users, such LLM agents would also need to be able to negotiate. In this paper, we study how well LLMs can negotiate with each other. We develop NegotiationArena: a flexible framework for evaluating and probing the negotiation abilities of LLM agents. We implemented three types of scenarios in NegotiationArena to assess LLM's behaviors in allocating shared resources (ultimatum games), aggregate resources (trading games) and buy/sell goods (price negotiations). Each scenario allows for multiple turns of flexible dialogues between LLM agents to allow for more complex negotiations. Interestingly, LLM agents can significantly boost their negotiation outcomes by employing certain behavioral tactics. For example, by pretending to be desolate and desperate, LLMs can improve their payoffs by 20{\textbackslash}\% when negotiating against the standard GPT-4. We also quantify irrational negotiation behaviors exhibited by the LLM agents, many of which also appear in humans. Together, {\textbackslash}NegotiationArena offers a new environment to investigate LLM interactions, enabling new insights into LLM's theory of mind, irrationality, and reasoning abilities.},
	urldate = {2024-03-13},
	publisher = {arXiv},
	author = {Bianchi, Federico and Chia, Patrick John and Yuksekgonul, Mert and Tagliabue, Jacopo and Jurafsky, Dan and Zou, James},
	month = feb,
	year = {2024},
	note = {arXiv:2402.05863 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Science and Game Theory},
	file = {arXiv Fulltext PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\WIK78TVN\\Bianchi et al. - 2024 - How Well Can LLMs Negotiate NegotiationArena Plat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\XJJXQE2D\\2402.html:text/html;How Well Can LLMs Negotiate_NEGOTIATIONARENA Platform and Analysis.pdf:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\V47UNCVW\\How Well Can LLMs Negotiate_NEGOTIATIONARENA Platform and Analysis.pdf:application/pdf},
}

@inproceedings{markel_gpteach_2023,
	address = {New York, NY, USA},
	series = {L@{S} '23},
	title = {{GPTeach}: {Interactive} {TA} {Training} with {GPT}-based {Students}},
	isbn = {9798400700255},
	shorttitle = {{GPTeach}},
	url = {https://doi.org/10.1145/3573051.3593393},
	doi = {10.1145/3573051.3593393},
	abstract = {Interactive and realistic teacher training is hard to scale. This is a key issue for learning at scale, as inadequate preparation can negatively impact both students and teachers. What if we could make the teacher training experience more engaging and, as a downstream effect, reduce the potential for harm that teachers-in-training could inflict on students? We present GPTeach, an interactive chat-based teacher training tool that allows novice teachers to practice with simulated students. We performed two studies to evaluate GPTeach: one think-aloud study and one A/B test between our tool and a baseline. Participants took the role of a teaching assistant conducting office hours with two GPT-simulated students. We found that our tool provides the opportunity for teachers to get valuable teaching practice without the pressures of affecting real students, allowing them to iterate their responses both during and across sessions. Additionally, participants enjoyed flexibility in tailoring their responses according to the varied personas, needs, and learning goals. In this paper, we provide quantitative results and qualitative observations to inform future work in this area. We conclude with a discussion of actionable design ideas for such systems, as well as other ways to use this tool for evaluating teachers and students. GPTeach has recently been deployed into the teacher training component of an online course with over 800 novice teachers.},
	urldate = {2024-03-14},
	booktitle = {Proceedings of the {Tenth} {ACM} {Conference} on {Learning} @ {Scale}},
	publisher = {Association for Computing Machinery},
	author = {Markel, Julia M. and Opferman, Steven G. and Landay, James A. and Piech, Chris},
	month = jul,
	year = {2023},
	keywords = {GPT-simulated students, scalable teacher training},
	pages = {226--236},
	file = {Full Text:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\9N9WSGQL\\Markel et al. - 2023 - GPTeach Interactive TA Training with GPT-based St.pdf:application/pdf},
}

@misc{hamilton_blind_2023,
	title = {Blind {Judgement}: {Agent}-{Based} {Supreme} {Court} {Modelling} {With} {GPT}},
	shorttitle = {Blind {Judgement}},
	url = {http://arxiv.org/abs/2301.05327},
	doi = {10.48550/arXiv.2301.05327},
	abstract = {We present a novel Transformer-based multi-agent system for simulating the judicial rulings of the 2010-2016 Supreme Court of the United States. We train nine separate models with the respective authored opinions of each supreme justice active ca. 2015 and test the resulting system on 96 real-world cases. We find our system predicts the decisions of the real-world Supreme Court with better-than-random accuracy. We further find a correlation between model accuracy with respect to individual justices and their alignment between legal conservatism \& liberalism. Our methods and results hold significance for researchers interested in using language models to simulate politically-charged discourse between multiple agents.},
	urldate = {2024-03-14},
	publisher = {arXiv},
	author = {Hamilton, Sil},
	month = jan,
	year = {2023},
	note = {arXiv:2301.05327 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: To appear in the proceedings of the AAAI-23 Workshop on Creative AI Across Modalities},
	file = {arXiv Fulltext PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\4TKPDPLT\\Hamilton - 2023 - Blind Judgement Agent-Based Supreme Court Modelli.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\KUNKZF34\\2301.html:text/html},
}

@article{bazerman_negotiation_2000,
	title = {Negotiation},
	volume = {51},
	issn = {0066-4308, 1545-2085},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.psych.51.1.279},
	doi = {10.1146/annurev.psych.51.1.279},
	abstract = {The first part of this paper traces a short history of the psychological study of negotiation. Although negotiation was an active research topic within social psychology in the 1960s and 1970s, in the 1980s, the behavioral decision perspective dominated. The 1990s has witnessed a rebirth of social factors in the psychological study of negotiation, including social relationships, egocentrism, motivated illusions, and emotion. The second part of this paper reviews five emerging research areas, each of which provides useful insight into how negotiators subjectively understand the negotiation: (a) mental models in negotiation; (b) how concerns of ethics, fairness, and values define the rules of the game being played; (c) how the selection of a communication medium impacts the way the game is played; (d) how cross-cultural issues in perception and behavior affect the negotiation game; and (e) how negotiators organize and simplify their understandings of the negotiation game when more than two actors are involved.},
	language = {en},
	number = {1},
	urldate = {2024-03-14},
	journal = {Annual Review of Psychology},
	author = {Bazerman, Max H. and Curhan, Jared R. and Moore, Don A. and Valley, Kathleen L.},
	month = feb,
	year = {2000},
	pages = {279--314},
}

@misc{zheng_judging_2023,
	title = {Judging {LLM}-as-a-{Judge} with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	doi = {10.48550/arXiv.2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
	urldate = {2024-03-14},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = dec,
	year = {2023},
	note = {arXiv:2306.05685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: NeurIPS 2023 Datasets and Benchmarks Track},
	file = {arXiv Fulltext PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\RNTELP5N\\Zheng et al. - 2023 - Judging LLM-as-a-Judge with MT-Bench and Chatbot A.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\QHCEM2R3\\2306.html:text/html},
}

@article{binz_using_2023,
	title = {Using cognitive psychology to understand {GPT}-3},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2218523120},
	doi = {10.1073/pnas.2218523120},
	abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3{\textquoteright}s decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3{\textquoteright}s behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
	language = {en},
	number = {6},
	urldate = {2024-03-14},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Binz, Marcel and Schulz, Eric},
	month = feb,
	year = {2023},
	pages = {e2218523120},
	file = {Binz and Schulz - 2023 - Using cognitive psychology to understand GPT-3.pdf:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\W8HP8FHH\\Binz and Schulz - 2023 - Using cognitive psychology to understand GPT-3.pdf:application/pdf},
}

@misc{noauthor_zhengrmixtao-7bx2-moe-instruct-v70_nodate,
	title = {zhengr/{MixTAO}-{7Bx2}-{MoE}-{Instruct}-v7.0 {\textperiodcentered} {Hugging} {Face}},
	url = {https://huggingface.co/zhengr/MixTAO-7Bx2-MoE-Instruct-v7.0},
	abstract = {We{\textquoteright}re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-03-14},
	file = {Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\FZW3RMET\\MixTAO-7Bx2-MoE-Instruct-v7.html:text/html},
}

@misc{noauthor_yunconglongtruthful_dpo_tomgrc_fusionnet_7bx2_moe_13b_nodate,
	title = {yunconglong/{Truthful}\_DPO\_TomGrc\_FusionNet\_7Bx2\_MoE\_13B {\textperiodcentered} {Hugging} {Face}},
	url = {https://huggingface.co/yunconglong/Truthful_DPO_TomGrc_FusionNet_7Bx2_MoE_13B},
	abstract = {We{\textquoteright}re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-03-14},
	file = {Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\T3A6QFWJ\\Truthful_DPO_TomGrc_FusionNet_7Bx2_MoE_13B.html:text/html},
}

@misc{noauthor_morehmomo-72b-lora-186-dpo_nodate,
	title = {moreh/{MoMo}-{72B}-lora-1.8.6-{DPO} {\textperiodcentered} {Hugging} {Face}},
	url = {https://huggingface.co/moreh/MoMo-72B-lora-1.8.6-DPO},
	abstract = {We{\textquoteright}re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-03-14},
	file = {Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\BKX2VL2W\\MoMo-72B-lora-1.8.html:text/html},
}

@misc{noauthor_jan-hqstealth-v2_nodate,
	title = {jan-hq/stealth-v2 {\textperiodcentered} {Hugging} {Face}},
	url = {https://huggingface.co/jan-hq/stealth-v2},
	abstract = {We{\textquoteright}re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-03-14},
	file = {Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\CELULQIP\\stealth-v2.html:text/html},
}

@misc{noauthor_mistralaimixtral-8x7b-instruct-v01_nodate,
	title = {mistralai/{Mixtral}-{8x7B}-{Instruct}-v0.1 {\textperiodcentered} {Hugging} {Face}},
	url = {https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1},
	abstract = {We{\textquoteright}re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-03-14},
	file = {Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\QCY5NGAF\\Mixtral-8x7B-Instruct-v0.html:text/html},
}

@misc{noauthor_abideenalphamonarch-laser_2024,
	title = {abideen/{AlphaMonarch}-laser {\textperiodcentered} {Hugging} {Face}},
	url = {https://huggingface.co/abideen/AlphaMonarch-laser},
	abstract = {We{\textquoteright}re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-03-14},
	month = feb,
	year = {2024},
	file = {Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\Q9XY4JPV\\AlphaMonarch-laser.html:text/html},
}

@misc{noauthor_meta-llamallama-2-70b-chat-hf_nodate,
	title = {meta-llama/{Llama}-2-70b-chat-hf {\textperiodcentered} {Hugging} {Face}},
	url = {https://huggingface.co/meta-llama/Llama-2-70b-chat-hf},
	abstract = {We{\textquoteright}re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-03-14},
	file = {Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\S8SC6R54\\Llama-2-70b-chat-hf.html:text/html},
}
