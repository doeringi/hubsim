
@misc{cheng_compost_2023,
	title = {{CoMPosT}: {Characterizing} and {Evaluating} {Caricature} in {LLM} {Simulations}},
	shorttitle = {{CoMPosT}},
	url = {http://arxiv.org/abs/2310.11501},
	abstract = {Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.},
	urldate = {2023-10-26},
	publisher = {arXiv},
	author = {Cheng, Myra and Piccardi, Tiziano and Yang, Diyi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11501 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
	annote = {Comment: To appear at EMNLP 2023 (Main)},
	file = {arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\TKPLP3MY\\2310.html:text/html;Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\9FHTKEHC\\Cheng et al. - 2023 - CoMPosT Characterizing and Evaluating Caricature .pdf:application/pdf},
}

@misc{huang_benchmarking_2023,
	title = {Benchmarking {Large} {Language} {Models} {As} {AI} {Research} {Agents}},
	url = {http://arxiv.org/abs/2310.03302},
	doi = {10.48550/arXiv.2310.03302},
	abstract = {Scientific experimentation involves an iterative process of creating hypotheses, designing experiments, running experiments, and analyzing the results. Can we build AI research agents to perform these long-horizon tasks? To take a step towards building and evaluating research agents on such open-ended decision-making tasks, we focus on the problem of machine learning engineering: given a task description and a dataset, build a high-performing model. In this paper, we propose MLAgentBench, a suite of ML tasks for benchmarking AI research agents. Agents can perform actions like reading/writing files, executing code, and inspecting outputs. With these actions, agents could run experiments, analyze the results, and modify the code of entire machine learning pipelines, such as data processing, architecture, training processes, etc. The benchmark then automatically evaluates the agent's performance objectively over various metrics related to performance and efficiency. We also design an LLM-based research agent to automatically perform experimentation loops in such an environment. Empirically, we find that a GPT-4-based research agent can feasibly build compelling ML models over many tasks in MLAgentBench, displaying highly interpretable plans and actions. However, the success rates vary considerably; they span from almost 90{\textbackslash}\% on well-established older datasets to as low as 10{\textbackslash}\% on recent Kaggle Challenges -- unavailable during the LLM model's pretraining -- and even 0{\textbackslash}\% on newer research challenges like BabyLM. Finally, we identify several key challenges for LLM-based research agents such as long-term planning and hallucination. Our code is released at https://github.com/snap-stanford/MLAgentBench.},
	urldate = {2023-10-26},
	publisher = {arXiv},
	author = {Huang, Qian and Vora, Jian and Liang, Percy and Leskovec, Jure},
	month = oct,
	year = {2023},
	note = {arXiv:2310.03302 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\DR7NTXCT\\Huang et al. - 2023 - Benchmarking Large Language Models As AI Research .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\BNSMAJE9\\2310.html:text/html},
}

@misc{park_generative_2023,
	title = {Generative {Agents}: {Interactive} {Simulacra} of {Human} {Behavior}},
	shorttitle = {Generative {Agents}},
	url = {http://arxiv.org/abs/2304.03442},
	doi = {10.48550/arXiv.2304.03442},
	abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = aug,
	year = {2023},
	note = {arXiv:2304.03442 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	annote = {

Does the memory stream store one stream per Agent separately?


},
	file = {arXiv Fulltext PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\I7RR25F6\\Park et al. - 2023 - Generative Agents Interactive Simulacra of Human .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\F4XLAGCA\\2304.html:text/html},
}

@misc{tornberg_simulating_2023,
	title = {Simulating {Social} {Media} {Using} {Large} {Language} {Models} to {Evaluate} {Alternative} {News} {Feed} {Algorithms}},
	url = {http://arxiv.org/abs/2310.05984},
	abstract = {Social media is often criticized for amplifying toxic discourse and discouraging constructive conversations. But designing social media platforms to promote better conversations is inherently challenging. This paper asks whether simulating social media through a combination of Large Language Models (LLM) and Agent-Based Modeling can help researchers study how different news feed algorithms shape the quality of online conversations. We create realistic personas using data from the American National Election Study to populate simulated social media platforms. Next, we prompt the agents to read and share news articles - and like or comment upon each other's messages - within three platforms that use different news feed algorithms. In the first platform, users see the most liked and commented posts from users whom they follow. In the second, they see posts from all users - even those outside their own network. The third platform employs a novel "bridging" algorithm that highlights posts that are liked by people with opposing political views. We find this bridging algorithm promotes more constructive, non-toxic, conversation across political divides than the other two models. Though further research is needed to evaluate these findings, we argue that LLMs hold considerable potential to improve simulation research on social media and many other complex social settings.},
	urldate = {2023-10-12},
	publisher = {arXiv},
	author = {T{\"o}rnberg, Petter and Valeeva, Diliara and Uitermark, Justus and Bail, Christopher},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05984 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Computer Science - Social and Information Networks},
	file = {arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\EV2R2B4K\\2310.html:text/html;Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\DMQHR5TD\\T{\"o}rnberg et al. - 2023 - Simulating Social Media Using Large Language Model.pdf:application/pdf},
}

@misc{qian_communicative_2023,
	title = {Communicative {Agents} for {Software} {Development}},
	url = {http://arxiv.org/abs/2307.07924},
	abstract = {Software engineering is a domain characterized by intricate decision-making processes, often relying on nuanced intuition and consultation. Recent advancements in deep learning have started to revolutionize software engineering practices through elaborate designs implemented at various stages of software development. In this paper, we present an innovative paradigm that leverages large language models (LLMs) throughout the entire software development process, streamlining and unifying key processes through natural language communication, thereby eliminating the need for specialized models at each phase. At the core of this paradigm lies ChatDev, a virtual chat-powered software development company that mirrors the established waterfall model, meticulously dividing the development process into four distinct chronological stages: designing, coding, testing, and documenting. Each stage engages a team of agents, such as programmers, code reviewers, and test engineers, fostering collaborative dialogue and facilitating a seamless workflow. The chat chain acts as a facilitator, breaking down each stage into atomic subtasks. This enables dual roles, allowing for proposing and validating solutions through context-aware communication, leading to efficient resolution of specific subtasks. The instrumental analysis of ChatDev highlights its remarkable efficacy in software generation, enabling the completion of the entire software development process in under seven minutes at a cost of less than one dollar. It not only identifies and alleviates potential vulnerabilities but also rectifies potential hallucinations while maintaining commendable efficiency and cost-effectiveness. The potential of ChatDev unveils fresh possibilities for integrating LLMs into the realm of software development.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Qian, Chen and Cong, Xin and Liu, Wei and Yang, Cheng and Chen, Weize and Su, Yusheng and Dang, Yufan and Li, Jiahao and Xu, Juyuan and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
	month = aug,
	year = {2023},
	note = {arXiv:2307.07924 [cs]
version: 3},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Multiagent Systems},
	annote = {Comment: https://github.com/OpenBMB/ChatDev},
	file = {arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\MXZP7FKR\\2307.html:text/html;Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\UNB2UZHW\\Qian et al. - 2023 - Communicative Agents for Software Development.pdf:application/pdf},
}

@misc{wu_autogen_2023,
	title = {{AutoGen}: {Enabling} {Next}-{Gen} {LLM} {Applications} via {Multi}-{Agent} {Conversation}},
	shorttitle = {{AutoGen}},
	url = {https://arxiv.org/abs/2308.08155v2},
	abstract = {AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.},
	language = {en},
	urldate = {2023-10-10},
	journal = {arXiv.org},
	author = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and Awadallah, Ahmed Hassan and White, Ryen W. and Burger, Doug and Wang, Chi},
	month = aug,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\5GZ4PHI4\\Wu et al. - 2023 - AutoGen Enabling Next-Gen LLM Applications via Mu.pdf:application/pdf},
}

@misc{wang_rolellm_2023,
	title = {{RoleLLM}: {Benchmarking}, {Eliciting}, and {Enhancing} {Role}-{Playing} {Abilities} of {Large} {Language} {Models}},
	shorttitle = {{RoleLLM}},
	url = {http://arxiv.org/abs/2310.00746},
	abstract = {The advent of Large Language Models (LLMs) has paved the way for complex tasks such as role-playing, which enhances user interactions by enabling models to imitate various characters. However, the closed-source nature of state-of-the-art LLMs and their general-purpose training limit role-playing optimization. In this paper, we introduce RoleLLM, a framework to benchmark, elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four stages: (1) Role Profile Construction for 100 roles; (2) Context-Based Instruction Generation (Context-Instruct) for role-specific knowledge extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning open-source models along with role customization. By Context-Instruct and RoleGPT, we create RoleBench, the first systematic and fine-grained character-level benchmark dataset for role-playing with 168,093 samples. Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese), significantly enhancing role-playing abilities and even achieving comparable results with RoleGPT (using GPT-4).},
	urldate = {2023-11-17},
	publisher = {arXiv},
	author = {Wang, Zekun Moore and Peng, Zhongyuan and Que, Haoran and Liu, Jiaheng and Zhou, Wangchunshu and Wu, Yuhan and Guo, Hongcheng and Gan, Ruitong and Ni, Zehao and Zhang, Man and Zhang, Zhaoxiang and Ouyang, Wanli and Xu, Ke and Chen, Wenhu and Fu, Jie and Peng, Junran},
	month = oct,
	year = {2023},
	note = {arXiv:2310.00746 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 30 pages, repo at https://github.com/InteractiveNLP-Team/RoleLLM-public},
	file = {arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\CB68RPBV\\2310.html:text/html;Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\Q37ZTVJS\\Wang et al. - 2023 - RoleLLM Benchmarking, Eliciting, and Enhancing Ro.pdf:application/pdf},
}

@misc{wang_does_2023,
	title = {Does {Role}-{Playing} {Chatbots} {Capture} the {Character} {Personalities}? {Assessing} {Personality} {Traits} for {Role}-{Playing} {Chatbots}},
	shorttitle = {Does {Role}-{Playing} {Chatbots} {Capture} the {Character} {Personalities}?},
	url = {http://arxiv.org/abs/2310.17976},
	abstract = {The emergence of large-scale pretrained language models has revolutionized the capabilities of new AI application, especially in the realm of crafting chatbots with distinct personas. Given the "stimulus-response" nature of chatbots, this paper unveils an innovative open-ended interview-style approach for personality assessment on role-playing chatbots, which offers a richer comprehension of their intrinsic personalities. We conduct personality assessments on 32 role-playing chatbots created by the ChatHaruhi library, across both the Big Five and MBTI dimensions, and measure their alignment with human perception. Evaluation results underscore that modern role-playing chatbots based on LLMs can effectively portray personality traits of corresponding characters, with an alignment rate of 82.8\% compared with human-perceived personalities. Besides, we also suggest potential strategies for shaping chatbots' personalities. Hence, this paper serves as a cornerstone study for role-playing chatbots that intersects computational linguistics and psychology. Our resources are available at https://github.com/LC1332/Chat-Haruhi-Suzumiya},
	urldate = {2023-11-17},
	publisher = {arXiv},
	author = {Wang, Xintao and Tu, Quan and Fei, Yaying and Leng, Ziang and Li, Cheng},
	month = oct,
	year = {2023},
	note = {arXiv:2310.17976 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: A Personality Traits Test Over ChatHaruhi},
	file = {arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\Z2QXMVHX\\2310.html:text/html;Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\43BFMKDF\\Wang et al. - 2023 - Does Role-Playing Chatbots Capture the Character P.pdf:application/pdf},
}

@misc{davidson_evaluating_2024,
	title = {Evaluating {Language} {Model} {Agency} through {Negotiations}},
	url = {http://arxiv.org/abs/2401.04536},
	abstract = {Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source models are currently unable to complete these tasks; (ii) cooperative bargaining games prove challenging; and (iii) the most powerful models do not always "win".},
	urldate = {2024-01-11},
	publisher = {arXiv},
	author = {Davidson, Tim R. and Veselovsky, Veniamin and Josifoski, Martin and Peyrard, Maxime and Bosselut, Antoine and Kosinski, Michal and West, Robert},
	month = jan,
	year = {2024},
	note = {arXiv:2401.04536 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Code and link to project data are made available at https://github.com/epfl-dlab/LAMEN},
	file = {arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\PRSJ9DG6\\2401.html:text/html;Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\UXKWY6ZM\\Davidson et al. - 2024 - Evaluating Language Model Agency through Negotiati.pdf:application/pdf},
}

@article{shanahan_role_2023,
	title = {Role play with large language models},
	volume = {623},
	copyright = {2023 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06647-8},
	doi = {10.1038/s41586-023-06647-8},
	abstract = {As dialogue agents become increasingly human-like in their performance, we must develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. Here we foreground the concept of role play. Casting dialogue-agent behaviour in terms of role play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models that they in fact lack. Two important cases of dialogue-agent behaviour are addressed this way, namely, (apparent) deception and (apparent) self-awareness.},
	language = {en},
	number = {7987},
	urldate = {2024-01-30},
	journal = {Nature},
	author = {Shanahan, Murray and McDonell, Kyle and Reynolds, Laria},
	month = nov,
	year = {2023},
	note = {Number: 7987
Publisher: Nature Publishing Group},
	keywords = {Computer science, Philosophy},
	pages = {493--498},
	file = {Full Text PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\E4EAA8WC\\Shanahan et al. - 2023 - Role play with large language models.pdf:application/pdf},
}

@misc{liu_agentbench_2023,
	title = {{AgentBench}: {Evaluating} {LLMs} as {Agents}},
	shorttitle = {{AgentBench}},
	url = {http://arxiv.org/abs/2308.03688},
	doi = {10.48550/arXiv.2308.03688},
	abstract = {Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Training on code and high quality multi-turn alignment data could improve agent performance. Datasets, environments, and an integrated evaluation package for AgentBench are released at {\textbackslash}url\{https://github.com/THUDM/AgentBench\}.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and Zhang, Shudan and Deng, Xiang and Zeng, Aohan and Du, Zhengxiao and Zhang, Chenhui and Shen, Sheng and Zhang, Tianjun and Su, Yu and Sun, Huan and Huang, Minlie and Dong, Yuxiao and Tang, Jie},
	month = oct,
	year = {2023},
	note = {arXiv:2308.03688 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 55 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\7HCQP8LB\\Liu et al. - 2023 - AgentBench Evaluating LLMs as Agents.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NicoD{\"o}ring\\Zotero\\storage\\KGE6BU6I\\2308.html:text/html},
}

@inproceedings{zhang_trojaning_2021,
	title = {Trojaning {Language} {Models} for {Fun} and {Profit}},
	url = {https://ieeexplore.ieee.org/abstract/document/9581257},
	doi = {10.1109/EuroSP51992.2021.00022},
	abstract = {Recent years have witnessed the emergence of a new paradigm of building natural language processing (NLP) systems: general-purpose, pre-trained language models (LMs) are composed with simple downstream models and fine-tuned for a variety of NLP tasks. This paradigm shift significantly simplifies the system development cycles. However, as many LMs are provided by untrusted third parties, their lack of standardization or regulation entails profound security implications, which are largely unexplored. To bridge this gap, this work studies the security threats posed by malicious LMs to NLP systems. Specifically, we present TrojanLM, a new class of trojaning attacks in which maliciously crafted LMs trigger host NLP systems to malfunction in a highly predictable manner. By empirically studying three state-of-the-art LMs (BERT, GPT-2, XLNet) in a range of security-critical NLP tasks (toxic comment detection, question answering, text completion) as well as user studies on crowdsourcing platforms, we demonstrate that TrojanLM possesses the following properties: (i) flexibility - the adversary is able to flexibly define logical combinations (e.g., {\textquoteleft}and{\textquoteright}, {\textquoteleft}or{\textquoteright}, {\textquoteleft}xor{\textquoteright}) of arbitrary words as triggers, (ii) efficacy - the host systems misbehave as desired by the adversary with high probability when {\textquotedblleft}trigger{\textquotedblright} -embedded inputs are present, (iii) specificity - the trojan LMs function indistinguishably from their benign counterparts on clean inputs, and (iv) fluency - the trigger-embedded inputs appear as fluent natural language and highly relevant to their surrounding contexts. We provide analytical justification for the practicality of TrojanLM, and further discuss potential countermeasures and their challenges, which lead to several promising research directions.},
	urldate = {2024-02-05},
	booktitle = {2021 {IEEE} {European} {Symposium} on {Security} and {Privacy} ({EuroS}\&{P})},
	author = {Zhang, Xinyang and Zhang, Zheng and Ji, Shouling and Wang, Ting},
	month = sep,
	year = {2021},
	keywords = {Natural language processing, Bridges, Buildings, Crowdsourcing, Knowledge discovery, Regulation, Standardization},
	pages = {179--197},
}
